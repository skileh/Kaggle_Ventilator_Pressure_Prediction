# -*- coding: utf-8 -*-
"""py-brain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cAuV9pvyI6i6Vo_Lk77UDAvPBVAiWyh
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import numpy as np
import pandas as pd
import os
import re
from sklearn.metrics import explained_variance_score
from sklearn.ensemble import RandomForestRegressor 
from sklearn.metrics import mean_squared_error
import seaborn as sns
from scipy import stats
from sklearn.covariance import MinCovDet
from scipy.sparse.linalg import inv
import scipy as sp
from scipy import linalg

df=pd.read_csv(r"/content/drive/MyDrive/kaggle/train.csv",encoding='latin-1')

df_teste =pd.read_csv(r"/content/drive/MyDrive/kaggle/test.csv",encoding='latin-1')

corpus_teste=pd.read_csv(r"/content/drive/MyDrive/kaggle/test.csv",encoding='latin-1')

#df = df.drop('breath_id',axis='columns')
df = df.drop('id',axis='columns')

#df_teste = df_teste.drop('breath_id',axis='columns')
df_teste = df_teste.drop('id',axis='columns')

#X = X.drop('breath_id',axis='columns')
np.random.seed(50)
x_treino, x_teste = train_test_split (df, test_size = 0.20, random_state = 42)

# Distância de Mahalanobis
from scipy.stats import chi2
import scipy.stats 
def mahalanobis_method(df):
    # MD
    x_minus_mu = df - np.mean(df)
    cov = np.cov(df.values.T)                           # Covariância
    inv_covmat = sp.linalg.inv(cov)                     # Covariância inversa
    left_term = np.dot(x_minus_mu, inv_covmat) 
    mahal = np.dot(left_term, x_minus_mu.T)
    md = np.sqrt(mahal.diagonal())
    
    # Marcação como outlier
    outlier = []
    #Cut-off point
    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))    # graus de liberdade = número de variáveis
    for index, value in enumerate(md):
        if value > C:
            outlier.append(index)
        else:
            continue
    return outlier, md

#df_bivariate = df.sample(n=20000)

#outliers_mahal_bi, md_bi = mahalanobis_method(df=df_bivariate)
#[380, 398, 404, 405, 410, 414, 418, 427]

#outliers_mahal, md = mahalanobis_method(df=df)

#df_bivariate = df_bivariate.reset_index(drop=True)

# Distância de Mahalanobis robusta
from scipy.stats import chi2
import scipy.stats 
def robust_mahalanobis_method(df):
    # Covariância de Determinante Mínimo
    rng = np.random.RandomState(0)
    real_cov = np.cov(df.values.T)
    X = rng.multivariate_normal(mean=np.mean(df, axis=0), cov=real_cov, size=506)
    cov = MinCovDet(random_state=0).fit(X)
    mcd = cov.covariance_ #robust covariance metric
    robust_mean = cov.location_  #robust mean
    inv_covmat = sp.linalg.inv(mcd) #inverse covariance metric
    
    # MD Robusta
    x_minus_mu = df - robust_mean
    left_term = np.dot(x_minus_mu, inv_covmat)
    mahal = np.dot(left_term, x_minus_mu.T)
    md = np.sqrt(mahal.diagonal())
    
    # Marcação como outlier
    outlier = []
    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))#degrees of freedom = number of variables
    for index, value in enumerate(md):
        if value > C:
            outlier.append(index)
        else:
            continue
    return outlier, md

#df_bivariate = df.sample(n=30000)

#outliers_mahal_rob_bi, md_rb_bi = robust_mahalanobis_method(df=df_bivariate)

# Método MAD
from scipy import stats
from statsmodels import robust  
def mad_method(df, variable_name):
    df = df.reset_index(drop=True)
    # Recebe dois parâmetros: dataframe & variável de interesse como string
    columns = df.columns
    med = np.median(df, axis = 0)
    mad = np.abs(robust.mad(df,c=1.0))
    threshold = 3
    outlier = []
    index=0
    for item in range(len(columns)):
        if columns[item] == variable_name:
            index == item
    for i, v in enumerate(df.loc[:,variable_name]):
        t = (v-med[index])/mad[index]
        if t > threshold:
            outlier.append(i)
        else:
            continue
    df = df.drop(outlier)
    return df

#data_outlier_mad = mad_method(x_teste, 'u_out')
#print(data_outlier_mad)

df

#Inversely proportional quantities
def outlider(df,text):
  df=df.sort_values(text)
  Q1=df[text].quantile(q=0.25)
  Q3=df[text].quantile(q=0.75)

  FIQ = Q3-Q1

  LF = Q1 - 1.5 * FIQ
  LS = Q3 + 1.5 * FIQ

  return df.loc[(df[text] >= LF) & (df[text] <= LS)]
def distanceRC(data):
  df = data
  array = []
  df_concat = pd.DataFrame()
  for i in range(df['breath_id'].size-1):
    df_aux = df.loc[(df['breath_id'] == i) & (df['u_in'] == 0) & (df['u_out'] == 1)]
    df = pd.concat([df_aux,df]).drop_duplicates(keep=False)
    df_aux=outlider(df_aux, 'pressure')
    df_concat = pd.concat([df_concat,df_aux])
  return df_concat

#df_final = df.sample(n=1000)    
data_outlier_mad = distanceRC(x_teste)
#data_outlier_mad.sample(n=5)

data_outlier_mad.to_csv('/content/drive/MyDrive/kaggle/datas.csv',index = False)

#x_new_test =  distanceRC(x_teste)
#x_new_test.sample(n=5)

x_train_x = data_outlier_mad.drop('pressure',axis='columns')
y_train = data_outlier_mad['pressure']

x_teste_x = x_teste.drop('pressure',axis='columns')
y_test = x_teste['pressure']

Q1=x_train_x['u_in'].quantile(q=0.25)
Q3=x_train_x['u_in'].quantile(q=0.75)

FIQ = Q3-Q1

LF = Q1 - 1.5 * FIQ
LS = Q3 + 1.5 * FIQ

print('FIQ = ', FIQ)
print('LF = ', LF)
print('LS = ', LS)

#ax = sns.scatterplot(x="R", y="C", data=x_treino_x)

x_train_x = x_train_x.drop('breath_id',axis='columns')
x_teste_x = x_teste_x.drop('breath_id',axis='columns')

RF=RandomForestRegressor(max_depth=6, random_state=0)
modelo_treinado=RF.fit(x_train_x,y_train)

predicoes = modelo_treinado.predict(x_teste_x)
score=modelo_treinado.score(x_teste_x,y_test)
print(score)
print(explained_variance_score(y_test, predicoes))
mean_squared_error (y_test, predicoes)

import xgboost as xgb

xgbr = xgb.XGBRegressor() 
modelo_treinado=xgbr.fit(x_train_x,y_train)

predicoes = modelo_treinado.predict(x_teste_x)
score=modelo_treinado.score(x_teste_x,y_test)
print(score)
print(explained_variance_score(y_test, predicoes))
mean_squared_error (y_test, predicoes)

print(x_teste_x)

from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500)

modelo_treinado=regr.fit(x_treino,y_treino)

predicoes = modelo_treinado.predict(x_teste)
score=modelo_treinado.score(x_teste,y_teste)
print(score)
print(explained_variance_score(y_teste, predicoes))
mean_squared_error (y_teste, predicoes)

import xgboost as xgb
X = df_soma.drop('pressure',axis='columns')
Y = df_soma['pressure']

xgbr = xgb.XGBRegressor(verbosity=0) 
modelo_final=xgbr.fit(X,Y)

predicao = modelo_final.predict(df_teste)

Id = corpus_teste['id'] 
submit_df_1 = pd.DataFrame ({ 
                   "id": Id, 
                  "pressure": predicao})

submit_df_1.to_csv('/content/drive/MyDrive/kaggle/submission.csv',index = False)

submit_df_1.sample(n=5)